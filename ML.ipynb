{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srichandana498/ML/blob/main/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WEEK - 1"
      ],
      "metadata": {
        "id": "qluljkNMHC5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOHKGRtgFzky",
        "outputId": "a6f1e01e-a642-4013-e6b3-4cc65fd8594e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sales DataFrame:\n",
            "             2014   2015    2016   2017\n",
            "Madhu      100.5  12000   20000  50000\n",
            "Kusum      150.8  18000   50000  60000\n",
            "Kinshuk    200.9  22000   70000  70000\n",
            "Ankit    30000.0  30000  100000  80000\n",
            "Shruti   40000.0  45000  125000  90000\n"
          ]
        }
      ],
      "source": [
        "## --- 9 ---\n",
        "import pandas as pd\n",
        "data = {\n",
        "    \"2014\": [100.5, 150.8, 200.9, 30000, 40000],\n",
        "    \"2015\": [12000, 18000, 22000, 30000, 45000],\n",
        "    \"2016\": [20000, 50000, 70000, 100000, 125000],\n",
        "    \"2017\": [50000, 60000, 70000, 80000, 90000]\n",
        "}\n",
        "index = [\"Madhu\", \"Kusum\", \"Kinshuk\", \"Ankit\", \"Shruti\"]\n",
        "\n",
        "sales = pd.DataFrame(data, index=index)\n",
        "print(\"Sales DataFrame:\\n\", sales)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgTdyPC2Fzh_",
        "outputId": "dbb9bb19-b300-406f-b64e-77c275c767cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "a) ['Madhu', 'Kusum', 'Kinshuk', 'Ankit', 'Shruti']\n",
            "\n",
            "b) ['2014', '2015', '2016', '2017']\n",
            "\n",
            "c)\n",
            " 2014    float64\n",
            "2015      int64\n",
            "2016      int64\n",
            "2017      int64\n",
            "dtype: object\n",
            "\n",
            "d)\n",
            "Dimensions (ndim): 2\n",
            "Shape: (5, 4)\n",
            "Size: 20\n",
            "Values:\n",
            " [[1.005e+02 1.200e+04 2.000e+04 5.000e+04]\n",
            " [1.508e+02 1.800e+04 5.000e+04 6.000e+04]\n",
            " [2.009e+02 2.200e+04 7.000e+04 7.000e+04]\n",
            " [3.000e+04 3.000e+04 1.000e+05 8.000e+04]\n",
            " [4.000e+04 4.500e+04 1.250e+05 9.000e+04]]\n",
            "\n",
            "e)\n",
            "            2014   2015    2016   2017\n",
            "Ankit   30000.0  30000  100000  80000\n",
            "Shruti  40000.0  45000  125000  90000\n",
            "\n",
            "f)\n",
            "             2014   2015\n",
            "Madhu      100.5  12000\n",
            "Kusum      150.8  18000\n",
            "Kinshuk    200.9  22000\n",
            "Ankit    30000.0  30000\n",
            "Shruti   40000.0  45000\n",
            "\n",
            "g)\n",
            "            2018\n",
            "Madhu    160000\n",
            "Kusum    110000\n",
            "Kinshuk  500000\n",
            "Ankit    340000\n",
            "Shruti   900000\n",
            "\n",
            "h) False\n"
          ]
        }
      ],
      "source": [
        "## ---10---\n",
        "import pandas as pd\n",
        "\n",
        "#\n",
        "data = {\n",
        "    \"2014\": [100.5, 150.8, 200.9, 30000, 40000],\n",
        "    \"2015\": [12000, 18000, 22000, 30000, 45000],\n",
        "    \"2016\": [20000, 50000, 70000, 100000, 125000],\n",
        "    \"2017\": [50000, 60000, 70000, 80000, 90000]\n",
        "}\n",
        "index = [\"Madhu\", \"Kusum\", \"Kinshuk\", \"Ankit\", \"Shruti\"]\n",
        "sales = pd.DataFrame(data, index=index)\n",
        "\n",
        "# a)\n",
        "print(\"\\na)\", sales.index.tolist())\n",
        "\n",
        "# b)\n",
        "print(\"\\nb)\", sales.columns.tolist())\n",
        "\n",
        "# c)\n",
        "print(\"\\nc)\\n\", sales.dtypes)\n",
        "\n",
        "# d)\n",
        "print(\"\\nd)\")\n",
        "print(\"Dimensions (ndim):\", sales.ndim)\n",
        "print(\"Shape:\", sales.shape)\n",
        "print(\"Size:\", sales.size)\n",
        "print(\"Values:\\n\", sales.values)\n",
        "\n",
        "# e)\n",
        "print(\"\\ne)\\n\", sales.tail(2))\n",
        "\n",
        "# f)\n",
        "print(\"\\nf)\\n\", sales.iloc[:, :2])\n",
        "\n",
        "# g)\n",
        "sales2 = pd.DataFrame({\n",
        "    \"2018\": [160000, 110000, 500000, 340000, 900000]\n",
        "}, index=[\"Madhu\", \"Kusum\", \"Kinshuk\", \"Ankit\", \"Shruti\"])\n",
        "print(\"\\ng)\\n\", sales2)\n",
        "\n",
        "# h)\n",
        "print(\"\\nh)\", sales2.empty)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPUu8BfpFzfX",
        "outputId": "4195a1e8-48c1-4f44-f2f9-7b9b4e5aecb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c)\n",
            " Madhu      50000.0\n",
            "Kusum      60000.0\n",
            "Kinshuk    70000.0\n",
            "Ankit      80000.0\n",
            "Shruti     90000.0\n",
            "Name: 2017, dtype: float64\n",
            "d)\n",
            "          Madhu     Ankit\n",
            "2017   50000.0   80000.0\n",
            "2018  160000.0  340000.0\n",
            "e)\n",
            " 125000.0\n",
            "\n",
            "l)\n",
            "       Shailesh     Kusum    Vivaan    Shruti   Sumeet\n",
            "2015   12000.0   18000.0   30000.0   45000.0  37800.0\n",
            "2016   20000.0   50000.0  100000.0  125000.0  52000.0\n",
            "2017   50000.0   60000.0   80000.0   90000.0  78438.0\n",
            "2018  100000.0  110000.0  340000.0  900000.0  38852.0\n"
          ]
        }
      ],
      "source": [
        "## --11--\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = {\n",
        "    \"2014\": [100.5, 150.8, 200.9, 30000, 40000],\n",
        "    \"2015\": [12000, 18000, 22000, 30000, 45000],\n",
        "    \"2016\": [20000, 50000, 70000, 100000, 125000],\n",
        "    \"2017\": [50000, 60000, 70000, 80000, 90000]\n",
        "}\n",
        "index = [\"Madhu\", \"Kusum\", \"Kinshuk\", \"Ankit\", \"Shruti\"]\n",
        "sales = pd.DataFrame(data, index=index)\n",
        "\n",
        "# a)\n",
        "sales2 = pd.DataFrame({\n",
        "    \"2018\": [160000, 110000, 500000, 340000, 900000]\n",
        "}, index=[\"Madhu\", \"Kusum\", \"Kinshuk\", \"Ankit\", \"Shruti\"])\n",
        "sales = pd.concat([sales, sales2], axis=1)\n",
        "\n",
        "# b)\n",
        "sales = sales.T\n",
        "\n",
        "# c)\n",
        "print(\"c)\\n\", sales.loc[\"2017\"])\n",
        "\n",
        "# d)\n",
        "print(\"d)\\n\", sales.loc[[\"2017\", \"2018\"], [\"Madhu\", \"Ankit\"]])\n",
        "\n",
        "# e)\n",
        "print(\"e)\\n\", sales.loc[\"2016\", \"Shruti\"])\n",
        "\n",
        "# f)\n",
        "sales[\"Sumeet\"] = [196.2, 37800, 52000, 78438, 38852]\n",
        "\n",
        "# g)\n",
        "sales = sales.drop(\"2014\")\n",
        "\n",
        "# h)\n",
        "sales = sales.drop(columns=\"Kinshuk\")\n",
        "\n",
        "# i)\n",
        "sales = sales.rename(columns={\"Ankit\": \"Vivaan\", \"Madhu\": \"Shailesh\"})\n",
        "\n",
        "# j)\n",
        "sales.loc[\"2018\", \"Shailesh\"] = 100000\n",
        "\n",
        "# k)\n",
        "sales.to_csv(\"SalesFigures.csv\", index=False, header=False)\n",
        "\n",
        "# l)\n",
        "sales_retrieved = pd.read_csv(\"SalesFigures.csv\", header=None)\n",
        "sales_retrieved.index = sales.index\n",
        "sales_retrieved.columns = sales.columns\n",
        "\n",
        "print(\"\\nl)\\n\", sales_retrieved)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5ZL_00KZFzcs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "outputId": "c50e4da3-f6cd-4571-a61c-1d9527d410b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c) Maximum price of LG TV: 15000\n",
            "d) Total Rupees of all products: 85000\n",
            "e) Median USD of SONY products: 765.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pymysql'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2118797295.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mdf_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Rupees\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mysql+pymysql://username:password@localhost:3306/your_database\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mdf_sorted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"product_table_sorted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/util/deprecations.py\u001b[0m in \u001b[0;36mwarned\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m                         \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                     )\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/create.py\u001b[0m in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m                 \u001b[0mdbapi_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0mdbapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbapi_meth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdbapi_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0mdialect_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dbapi\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/dialects/mysql/pymysql.py\u001b[0m in \u001b[0;36mimport_dbapi\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mimport_dbapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDBAPIModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pymysql\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mlanghelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoized_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymysql'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "## Exercise 2\n",
        "## ---13---\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# a)\n",
        "data = {\n",
        "    \"Item\": [\"TV\", \"TV\", \"TV\", \"AC\"],\n",
        "    \"Company\": [\"LG\", \"VIDEOCON\", \"LG\", \"SONY\"],\n",
        "    \"Rupees\": [12000, 10000, 15000, 14000],\n",
        "    \"USD\": [700, 650, 800, 750]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# b)\n",
        "new_rows = pd.DataFrame([\n",
        "    {\"Item\": \"Fridge\", \"Company\": \"LG\", \"Rupees\": 18000, \"USD\": 850},\n",
        "    {\"Item\": \"AC\", \"Company\": \"SONY\", \"Rupees\": 16000, \"USD\": 780}\n",
        "])\n",
        "df = pd.concat([df, new_rows], ignore_index=True)\n",
        "\n",
        "# c)\n",
        "max_lg_tv = df[(df[\"Company\"] == \"LG\") & (df[\"Item\"] == \"TV\")][\"Rupees\"].max()\n",
        "print(\"c) Maximum price of LG TV:\", max_lg_tv)\n",
        "\n",
        "# d)\n",
        "total_rupees = df[\"Rupees\"].sum()\n",
        "print(\"d) Total Rupees of all products:\", total_rupees)\n",
        "\n",
        "# e)\n",
        "median_sony_usd = df[df[\"Company\"] == \"SONY\"][\"USD\"].median()\n",
        "print(\"e) Median USD of SONY products:\", median_sony_usd)\n",
        "\n",
        "# f)\n",
        "df_sorted = df.sort_values(by=\"Rupees\", ascending=True)\n",
        "\n",
        "engine = create_engine(\"mysql+pymysql://username:password@localhost:3306/your_database\")\n",
        "\n",
        "df_sorted.to_sql(name=\"product_table_sorted\", con=engine, if_exists=\"replace\", index=False)\n",
        "print(\"f) Sorted data transferred to MySQL.\")\n",
        "\n",
        "# g)\n",
        "df_new = pd.DataFrame({\n",
        "    \"Item\": [\"Washing Machine\", \"Microwave\"],\n",
        "    \"Company\": [\"Samsung\", \"Panasonic\"],\n",
        "    \"Rupees\": [22000, 12500],\n",
        "    \"USD\": [950, 610]\n",
        "})\n",
        "\n",
        "df_new.to_sql(name=\"new_product_table\", con=engine, if_exists=\"replace\", index=False)\n",
        "print(\"g) New product data transferred to MySQL.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5OSn6UTFzaH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# a)\n",
        "data = {\n",
        "    'Name': ['Anita', 'Ravi', 'Meena', 'Amit', 'Sana', 'Kunal'],\n",
        "    'Degree': ['BCA', 'MBA', 'BCA', 'MBA', 'BTech', 'MBA'],\n",
        "    'Marks': [85, np.nan, 91, 76, 88, np.nan]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# b)\n",
        "print(\"b) Maximum marks in each Degree:\")\n",
        "print(df.groupby('Degree')['Marks'].max())\n",
        "\n",
        "# c)\n",
        "df['Marks'] = df['Marks'].fillna(76)\n",
        "\n",
        "# d)\n",
        "df.set_index('Name', inplace=True)\n",
        "\n",
        "# e)\n",
        "print(\"\\ne) Name and Degree-wise average marks:\")\n",
        "print(df.groupby(['Name', 'Degree'])['Marks'].mean())\n",
        "\n",
        "# f)\n",
        "print(\"\\nf) Number of students in MBA:\")\n",
        "print((df['Degree'] == 'MBA').sum())\n",
        "\n",
        "# g)\n",
        "print(\"\\ng) Mode marks in BCA:\")\n",
        "print(df[df['Degree'] == 'BCA']['Marks'].mode())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WEEK - 2"
      ],
      "metadata": {
        "id": "vVnaMFUPA-wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AwXOaxYlGhm"
      },
      "outputs": [],
      "source": [
        "TITANIC DATASET\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "derfpsSpFzMZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import VarianceThreshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDXDjzEhFzDZ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/titanic_train.csv')\n",
        "df.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keqdtI8Ip_28"
      },
      "outputs": [],
      "source": [
        "[col for col in df.columns if df[col].isnull().sum() > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjgiZ5Elp_0c"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df.drop(labels=['Survived'], axis=1), df['Survived'], test_size=0.3, random_state=0)\n",
        "x_train.shape, x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ6cgYY8p_xg"
      },
      "outputs": [],
      "source": [
        "constant_features_numeric = [feat for feat in x_train.select_dtypes(exclude='object').columns if x_train[feat].nunique() == 1]\n",
        "print(f\"Number of numeric constant features: {len(constant_features_numeric)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKFyvitYp_u3"
      },
      "outputs": [],
      "source": [
        "x_train.drop(labels=constant_features_numeric, axis=1, inplace=True)\n",
        "x_test.drop(labels=constant_features_numeric, axis=1, inplace=True)\n",
        "x_train.shape, x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEhzxh7Fp_cl"
      },
      "outputs": [],
      "source": [
        "quasi_constant_features = []\n",
        "for feature in x_train.columns:\n",
        "    predominant = (x_train[feature].value_counts() / float(len(x_train))).sort_values(ascending=False).values[0]\n",
        "    if predominant >= 0.998:\n",
        "        quasi_constant_features.append(feature)\n",
        "\n",
        "print(f\"Number of quasi-constant features: {len(quasi_constant_features)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbvKPETx0yci"
      },
      "outputs": [],
      "source": [
        "x_train.drop(labels=quasi_constant_features, axis=1, inplace=True)\n",
        "x_test.drop(labels=quasi_constant_features, axis=1, inplace=True)\n",
        "x_train.shape, x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw0aX2Se0yZ1"
      },
      "outputs": [],
      "source": [
        "duplicated_feat = []\n",
        "for i in range(0, len(x_train.columns)):\n",
        "    col_1 = x_train.columns[i]\n",
        "    for col_2 in x_train.columns[i + 1:]:\n",
        "        if x_train[col_1].equals(x_train[col_2]):\n",
        "            duplicated_feat.append(col_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7k38o6_z2rHf"
      },
      "outputs": [],
      "source": [
        "duplicated_features = set(duplicated_feat)\n",
        "print(f\"Number of duplicated features: {len(duplicated_features)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVc-nHHw0yXI"
      },
      "outputs": [],
      "source": [
        "x_train.drop(labels=duplicated_features, axis=1, inplace=True)\n",
        "x_test.drop(labels=duplicated_features, axis=1, inplace=True)\n",
        "x_train.shape, x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7EC3cLL0yRt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Select only numerical columns for correlation calculation\n",
        "corrmat = x_train.select_dtypes(include=np.number).corr()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "sns.heatmap(corrmat, vmax=1.0, square=True, cmap='coolwarm', annot=False, ax=ax)\n",
        "\n",
        "plt.title('Correlation Heatmap of Features')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHuJL1jD4kVR"
      },
      "outputs": [],
      "source": [
        "grouped_features = []\n",
        "correlated_features = set()\n",
        "def correlation(dataset, threshold):\n",
        "    column_corr = {}\n",
        "    # Select only numerical columns for correlation calculation\n",
        "    dataset_numeric = dataset.select_dtypes(include=np.number)\n",
        "    corr_matrix = dataset_numeric.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "                colname = corr_matrix.columns[i]\n",
        "                col_arr = column_corr.get(colname, [])\n",
        "                col_arr.append(corr_matrix.columns[j])\n",
        "                column_corr[colname] = col_arr\n",
        "                correlated_features.add(corr_matrix.columns[j])\n",
        "    return column_corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi7TeWP349-F"
      },
      "outputs": [],
      "source": [
        "corr_features = correlation(x_train, 0.9)\n",
        "correlated_features = set(filter(lambda x: x in x_train.columns, correlated_features))\n",
        "print(f\"Number of correlated features: {len(correlated_features)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs_f8gTt4kTG"
      },
      "outputs": [],
      "source": [
        "x_train.drop(labels=correlated_features, axis=1, inplace=True)\n",
        "x_test.drop(labels=correlated_features, axis=1, inplace=True)\n",
        "\n",
        "x_train.shape, x_test.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qyvv9x7O4kQi"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Select only numerical columns for mutual information calculation\n",
        "x_train_numeric = x_train.select_dtypes(include=np.number)\n",
        "\n",
        "\n",
        "mi = mutual_info_classif(x_train_numeric.fillna(0), y_train)\n",
        "\n",
        "mi = pd.Series(mi, index=x_train_numeric.columns)\n",
        "mi = mi.sort_values(ascending=False)\n",
        "\n",
        "mi.plot.bar(figsize=(20, 8))\n",
        "plt.title(\"Mutual Information Scores for Numeric Features\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcDyhJwM4kN5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import chi2_contingency\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Titanic subset with categorical & discrete vars\n",
        "variables = ['pclass', 'survived', 'sex', 'sibsp', 'parch', 'embarked']\n",
        "data = pd.read_csv(\n",
        "    'https://www.openml.org/data/get_csv/16826755/phpMYEkMl',\n",
        "    usecols=variables,\n",
        "    na_values='?'\n",
        ")\n",
        "\n",
        "data.dropna(subset=['embarked'], inplace=True)\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMJwoOx14kLO"
      },
      "outputs": [],
      "source": [
        "for col in ['pclass', 'sex', 'sibsp', 'parch', 'embarked']:\n",
        "    contingency_table = pd.crosstab(data[col], data['survived'])\n",
        "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "    print(f\"Feature: {col}, Chi2: {chi2:.2f}, p-value: {p:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sWpYzed6avC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import chi2_contingency\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "variables = ['pclass', 'survived', 'sex', 'sibsp', 'parch', 'embarked']\n",
        "data = pd.read_csv(\n",
        "    'https://www.openml.org/data/get_csv/16826755/phpMYEkMl',\n",
        "    usecols=variables,\n",
        "    na_values='?',\n",
        "    )\n",
        "data.dropna(subset=['embarked'], inplace=True)\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYbpArys6asd"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    data.drop(\"survived\", axis=1),\n",
        "    data['survived'],\n",
        "    test_size=0.3,\n",
        "    random_state=0,\n",
        ")\n",
        "\n",
        "\n",
        "c = pd.crosstab(y_train, x_train['sex'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiDTCnGUZi2H"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qfm2mA24aB5B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import VarianceThreshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-4iCTxIaNGQ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/titanic_train.csv')\n",
        "df.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjvDT1K8Wkyi"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    data.drop(\"Survived\", axis=1),\n",
        "    data['Survived'],\n",
        "    test_size=0.3,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "# One-hot encode categorical features (Chi-square needs categorical or integer)\n",
        "x_train_encoded = pd.get_dummies(x_train, drop_first=True)\n",
        "x_test_encoded = pd.get_dummies(x_test, drop_first=True)\n",
        "\n",
        "# Align columns between train and test\n",
        "x_train_encoded, x_test_encoded = x_train_encoded.align(x_test_encoded, join='left', axis=1, fill_value=0)\n",
        "\n",
        "chi_ls = []\n",
        "\n",
        "for feature in x_train_encoded.columns:\n",
        "    crosstab = pd.crosstab(y_train, x_train_encoded[feature])\n",
        "    chi2, p_value, dof, expected = chi2_contingency(crosstab)\n",
        "    chi_ls.append(p_value)\n",
        "\n",
        "\n",
        "pd.Series(chi_ls, index=x_train_encoded.columns) \\\n",
        "    .sort_values(ascending=True) \\\n",
        "    .plot.bar(rot=90, figsize=(9, 5))\n",
        "\n",
        "plt.ylabel(\"p value\")\n",
        "plt.title(\"Feature importance based on Chi-square test\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SANTANDER CUSTOMER SATISFACTION"
      ],
      "metadata": {
        "id": "vCz5EPlOBHca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neyJZwnkWkr-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_JR8jBijtUg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD2ZQMyzWkhW"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/Santander Customer Satisfaction_train.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ndN4x4KWkdq"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUadCkZYWkZU"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJwZ630ff8dZ"
      },
      "outputs": [],
      "source": [
        "nullColumns = [col for col in df.columns if df[col].isnull().sum()>0]\n",
        "nullColumns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emCs2ZMKf8a4"
      },
      "outputs": [],
      "source": [
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first = True)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1HmEbr8f70g"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_gurnmHf7yK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Santander Customer Satisfaction_test.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdtUs2T3f7ve"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Santander Customer Satisfaction_train (1).csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97ecda17"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUqq8_uzf7pP"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    df.drop(labels=['TARGET', 'ID'], axis=1),\n",
        "    df['TARGET'],\n",
        "    test_size=0.3,\n",
        "    random_state=0)\n",
        "\n",
        "x_train.shape, x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_2A305yf7lg"
      },
      "outputs": [],
      "source": [
        "def correlation(dataset, threshold):\n",
        "    col_corr = set()\n",
        "    corr_matrix = dataset.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "                colname = corr_matrix.columns[i]\n",
        "                col_corr.add(colname)\n",
        "    return col_corr\n",
        "corr_features = correlation(x_train, 0.8)\n",
        "print('correlated features: ', len(set(corr_features)) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_VxSBwztgZH"
      },
      "outputs": [],
      "source": [
        "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "efs = EFS(estimator=lr,\n",
        "          min_features=1,\n",
        "          max_features=4,\n",
        "          scoring='accuracy',\n",
        "          cv=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRyxp9O_znMW"
      },
      "outputs": [],
      "source": [
        "efs = efs.fit(x_train.fillna(0), y_train)\n",
        "\n",
        "print('Best accuracy score: %.2f' % efs.best_score_)\n",
        "print('Best subset (indices):', efs.best_idx_)\n",
        "print('Best subset (corresponding names):', efs.best_feature_names_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gj3nKk7tgT0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "titanic = sns.load_dataset('titanic')\n",
        "numeric_data = titanic.select_dtypes(include=[np.number]).dropna()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(numeric_data)\n",
        "\n",
        "pca = PCA()\n",
        "pca_data = pca.fit_transform(scaled_data)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(pca_data[:, 0], pca_data[:, 1], alpha=0.7, c='dodgerblue', edgecolor='k')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA: PC1 vs PC2 (Titanic Dataset)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "loadings = pd.DataFrame(pca.components_.T,\n",
        "                        columns=[f'PC{i+1}' for i in range(len(numeric_data.columns))],\n",
        "                        index=numeric_data.columns)\n",
        "\n",
        "pc1_top = loadings['PC1'].abs().sort_values(ascending=False)\n",
        "pc2_top = loadings['PC2'].abs().sort_values(ascending=False)\n",
        "\n",
        "print(\"\\n(c) Top variables contributing to PC1:\")\n",
        "print(pc1_top.head())\n",
        "\n",
        "print(\"\\n(c) Top variables contributing to PC2:\")\n",
        "print(pc2_top.head())\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "pc1_pc2_variance = explained_variance[0] + explained_variance[1]\n",
        "print(f\"\\n(d) Total variance explained by PC1 and PC2: {pc1_pc2_variance:.2%}\")\n",
        "\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "num_components_80 = np.argmax(cumulative_variance >= 0.80) + 1\n",
        "print(f\"(e) Number of principal components to explain at least 80% of variance: {num_components_80}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "usyqNrSQFFCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCKpFCl0tgOd"
      },
      "outputs": [],
      "source": [
        "features_to_drop = list(set(corr_features) & set(x_train.columns))\n",
        "\n",
        "x_train.drop(labels=features_to_drop, axis=1, inplace=True)\n",
        "x_test.drop(labels=features_to_drop, axis=1, inplace=True)\n",
        "x_train.shape, x_test.shape\n",
        "\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "sfs1 = SFS(RandomForestClassifier(n_jobs=4),\n",
        "           k_features=5,\n",
        "           forward=True,\n",
        "           verbose=2,\n",
        "           scoring='roc_auc',\n",
        "           cv=3)\n",
        "sfs1 = sfs1.fit(np.array(x_train.fillna(0)), y_train)\n",
        "\n",
        "print('Best accuracy score: %.2f' % sfs1.k_score_)\n",
        "print('Best subset (indices):', sfs1.k_feature_idx_)\n",
        "print('Best subset (corresponding names):', sfs1.k_feature_names_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKaNsDJGtgLm"
      },
      "outputs": [],
      "source": [
        "features_to_drop = list(set(corr_features) & set(x_train.columns))\n",
        "\n",
        "x_train.drop(labels=features_to_drop, axis=1, inplace=True)\n",
        "x_test.drop(labels=features_to_drop, axis=1, inplace=True)\n",
        "x_train.shape, x_test.shape\n",
        "\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "sfs1 = SFS(RandomForestClassifier(n_jobs=4),\n",
        "           k_features=5,\n",
        "           forward=True,\n",
        "           verbose=2,\n",
        "           scoring='roc_auc',\n",
        "           cv=3)\n",
        "sfs1 = sfs1.fit(np.array(x_train.fillna(0)), y_train)\n",
        "\n",
        "print('Best accuracy score: %.2f' % sfs1.k_score_)\n",
        "print('Best subset (indices):', sfs1.k_feature_idx_)\n",
        "print('Best subset (corresponding names):', sfs1.k_feature_names_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcTTYTAPtgIM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmyjlV56tgFs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3c778f7"
      },
      "source": [
        "def correlation(dataset, threshold):\n",
        "    col_corr = set()\n",
        "    corr_matrix = dataset.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "                colname = corr_matrix.columns[i]\n",
        "                col_corr.add(colname)\n",
        "    return col_corr\n",
        "corr_features = correlation(x_train, 0.8)\n",
        "print('correlated features: ', len(set(corr_features)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6aa9a93"
      },
      "source": [
        "features_to_drop = list(set(corr_features) & set(x_train.columns))\n",
        "\n",
        "x_train.drop(labels=features_to_drop, axis=1, inplace=True)\n",
        "x_test.drop(labels=features_to_drop, axis=1, inplace=True)\n",
        "x_train.shape, x_test.shape\n",
        "\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "sfs1 = SFS(RandomForestClassifier(n_jobs=4),\n",
        "           k_features=5,\n",
        "           forward=True,\n",
        "           verbose=2,\n",
        "           scoring='roc_auc',\n",
        "           cv=3)\n",
        "sfs1 = sfs1.fit(np.array(x_train.fillna(0)), y_train)\n",
        "\n",
        "print('Best accuracy score: %.2f' % sfs1.k_score_)\n",
        "print('Best subset (indices):', sfs1.k_feature_idx_)\n",
        "print('Best subset (corresponding names):', sfs1.k_feature_names_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5ecae3b"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    df.drop(labels=['TARGET', 'ID'], axis=1),\n",
        "    df['TARGET'],\n",
        "    test_size=0.3,\n",
        "    random_state=0)\n",
        "\n",
        "x_train.shape, x_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f816063"
      },
      "source": [
        "def correlation(dataset, threshold):\n",
        "    col_corr = set()\n",
        "    corr_matrix = dataset.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "                colname = corr_matrix.columns[i]\n",
        "                col_corr.add(colname)\n",
        "    return col_corr\n",
        "corr_features = correlation(x_train, 0.8)\n",
        "print('correlated features: ', len(set(corr_features)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c734d2ee"
      },
      "source": [
        "features_to_drop = list(set(corr_features) & set(x_train.columns))\n",
        "\n",
        "x_train.drop(labels=features_to_drop, axis=1, inplace=True)\n",
        "x_test.drop(labels=features_to_drop, axis=1, inplace=True)\n",
        "x_train.shape, x_test.shape\n",
        "\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "sfs1 = SFS(RandomForestClassifier(n_jobs=4),\n",
        "           k_features=5,\n",
        "           forward=True,\n",
        "           verbose=2,\n",
        "           scoring='roc_auc',\n",
        "           cv=3)\n",
        "sfs1 = sfs1.fit(np.array(x_train.fillna(0)), y_train)\n",
        "\n",
        "print('Best accuracy score: %.2f' % sfs1.k_score_)\n",
        "print('Best subset (indices):', sfs1.k_feature_idx_)\n",
        "print('Best subset (corresponding names):', sfs1.k_feature_names_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "447be335"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "381b5416"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    df.drop(labels=['TARGET', 'ID'], axis=1),\n",
        "    df['TARGET'],\n",
        "    test_size=0.3,\n",
        "    random_state=0)\n",
        "\n",
        "x_train.shape, x_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b493cff"
      },
      "source": [
        "def correlation(dataset, threshold):\n",
        "    col_corr = set()\n",
        "    corr_matrix = dataset.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "                colname = corr_matrix.columns[i]\n",
        "                col_corr.add(colname)\n",
        "    return col_corr\n",
        "corr_features = correlation(x_train, 0.8)\n",
        "print('correlated features: ', len(set(corr_features)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39d4b8ac"
      },
      "source": [
        "features_to_drop = list(set(corr_features) & set(x_train.columns))\n",
        "\n",
        "x_train.drop(labels=features_to_drop, axis=1, inplace=True)\n",
        "x_test.drop(labels=features_to_drop, axis=1, inplace=True)\n",
        "x_train.shape, x_test.shape\n",
        "\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "sfs1 = SFS(RandomForestClassifier(n_jobs=4),\n",
        "           k_features=5,\n",
        "           forward=True,\n",
        "           verbose=2,\n",
        "           scoring='roc_auc',\n",
        "           cv=3)\n",
        "sfs1 = sfs1.fit(np.array(x_train.fillna(0)), y_train)\n",
        "\n",
        "print('Best accuracy score: %.2f' % sfs1.k_score_)\n",
        "print('Best subset (indices):', sfs1.k_feature_idx_)\n",
        "print('Best subset (corresponding names):', sfs1.k_feature_names_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b06edf95"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Santander Customer Satisfaction_train (1).csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aee26ea"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97b5ff6b"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    df.drop(labels=['TARGET', 'ID'], axis=1),\n",
        "    df['TARGET'],\n",
        "    test_size=0.3,\n",
        "    random_state=0)\n",
        "\n",
        "x_train.shape, x_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97c639d7"
      },
      "source": [
        "def correlation(dataset, threshold):\n",
        "    col_corr = set()\n",
        "    corr_matrix = dataset.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "                colname = corr_matrix.columns[i]\n",
        "                col_corr.add(colname)\n",
        "    return col_corr\n",
        "corr_features = correlation(x_train, 0.8)\n",
        "print('correlated features: ', len(set(corr_features)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5090510"
      },
      "source": [
        "features_to_drop = list(set(corr_features) & set(x_train.columns))\n",
        "\n",
        "x_train.drop(labels=features_to_drop, axis=1, inplace=True)\n",
        "x_test.drop(labels=features_to_drop, axis=1, inplace=True)\n",
        "x_train.shape, x_test.shape\n",
        "\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "sfs1 = SFS(RandomForestClassifier(n_jobs=4),\n",
        "           k_features=5,\n",
        "           forward=True,\n",
        "           verbose=2,\n",
        "           scoring='roc_auc',\n",
        "           cv=3)\n",
        "sfs1 = sfs1.fit(np.array(x_train.fillna(0)), y_train)\n",
        "\n",
        "print('Best accuracy score: %.2f' % sfs1.k_score_)\n",
        "print('Best subset (indices):', sfs1.k_feature_idx_)\n",
        "print('Best subset (corresponding names):', sfs1.k_feature_names_)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNB+VnPfyLGeDrSmcYSchCW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}